{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "id": "mzZ9zQX06uK0"
      },
      "source": [
        "---\n",
        "title: \"Assignment 1: Exploring Word Vectors\"\n",
        "format:\n",
        "  html:\n",
        "    toc: true\n",
        "    toc-title: Contents\n",
        "    toc-depth: 4\n",
        "    self-contained: true\n",
        "    number-sections: false\n",
        "jupyter: python3\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HqK14JLSStR"
      },
      "source": [
        "### <font color='blue'> Due 11:59pm, Monday Januray 19th 2026</font>\n",
        "\n",
        "**Runtime / setup notes:**\n",
        "\n",
        "- This notebook does **not** require a GPU.\n",
        "- If you run locally on your laptop, you must set up the Python environment first (see the repo setup docs/README).\n",
        "- You can also run in **Google Colab**; if you do, install the dependencies whenever needed (e.g., `pip install gensim`).\n",
        "\n",
        "**Total points: 100**\n",
        "\n",
        "Welcome to STAT359!\n",
        "\n",
        "Before you start, make sure you **read the README.md** in the github repo for important setup information. You need to install some Python libraries before you can successfully do this assignment. A lot of code is provided in this notebook, and we highly encourage you to read and understand it as part of the learning :)\n",
        "\n",
        "If you aren't super familiar with Python, Numpy, or Matplotlib. The STAT303-1 [textbook chapters on Python/Numpy basics](https://lizhen0909.github.io/nu-stat303-1-sec20-coursebook/) are a great resource.\n",
        "\n",
        "\n",
        "**Assignment Notes:** Please make sure to save the notebook as you go along. Submission Instructions are located at the bottom of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-03-27T08:04:19.343709Z",
          "start_time": "2024-03-27T08:04:15.222676Z"
        },
        "id": "8AGQxROrSStf"
      },
      "outputs": [],
      "source": [
        "# Imports (do not add additional imports unless instructed)\n",
        "import re\n",
        "import random\n",
        "import pprint\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [10, 5]\n",
        "\n",
        "from datasets import load_dataset\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "random.seed(0)\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SICd5IedSSto"
      },
      "source": [
        "## Word Vectors\n",
        "\n",
        "Word vectors are a fundamental building block in NLP. They are used directly in many downstream tasks (e.g., classification, retrieval, and translation) and serve as inputs to sequence models—from early RNNs to modern Transformer-based LLMs. Because of this, it’s important to build intuition about their strengths and limitations.\n",
        "\n",
        "In this assignment, you will explore two families of word vectors: **count-based vectors** derived from *co-occurrence matrices*, and **GloVe**, which learns embeddings from global co-occurrence statistics using a training objective.\n",
        "\n",
        "\n",
        "**Note on Terminology:** The terms \"word vectors\" and \"word embeddings\" are often used interchangeably. The term \"embedding\" refers to the fact that we are encoding aspects of a word's meaning in a lower dimensional space. As [Wikipedia](https://en.wikipedia.org/wiki/Word_embedding) states, \"*conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with a much lower dimension*\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gpZZXtt6uK6"
      },
      "source": [
        "  ## Part 1: Count-Based Word Vectors (30 points)\n",
        "\n",
        "Most word vector models start from the following idea:\n",
        "\n",
        "> *“You shall know a word by the company it keeps.”* — ([Firth, J. R. 1957:11](https://en.wikipedia.org/wiki/John_Rupert_Firth))\n",
        "\n",
        "Many word vector approaches rely on the intuition that **semantically similar words** tend to appear in **similar contexts**. As a result, such words often **co-occur with overlapping sets of neighboring words**. By examining these co-occurrence patterns, we can learn vector representations (embeddings) that capture aspects of meaning.\n",
        "\n",
        "Historically, many classical methods constructed word vectors from **co-occurrence statistics**, rather than learning them through end-to-end prediction objectives. In this section, we focus on one of the most common count-based strategies: **co-occurrence matrices**.\n",
        "\n",
        "**Further reading:**\n",
        "\n",
        "- Jurafsky & Martin, *Speech and Language Processing* (Chapter 5): https://web.stanford.edu/~jurafsky/slp3/5.pdf\n",
        "- Overview article (archived): https://web.archive.org/web/20190530091127/https://medium.com/data-science-group-iitr/word-embedding-2d05d270b285\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6uQ9-DVSSts"
      },
      "source": [
        "### Co-Occurrence\n",
        "\n",
        "A **co-occurrence matrix** counts how often tokens appear near each other. Suppose a token $w_i$ appears at position $i$ in a document. With a fixed **window size** $n$, the *context* of $w_i$ consists of the $n$ tokens immediately to the left and the $n$ tokens immediately to the right:\n",
        "$$\n",
        "w_{i-n},\\dots,w_{i-1},\\; w_{i+1},\\dots,w_{i+n}.\n",
        "$$\n",
        "We build a word-by-word matrix $M$ where $M_{ij}$ is the number of times word $w_j$ appears in the context window of word $w_i$ (summed over the whole corpus). Many implementations make $M$ **symmetric** by counting co-occurrences in both directions; in that case $M_{ij}=M_{ji}$.\n",
        "\n",
        "**Example (window size $n=1$)**\n",
        "\n",
        "Document 1: `all that glitters is not gold`\n",
        "\n",
        "Document 2: `all is well that ends well`\n",
        "\n",
        "Below is the co-occurrence matrix for this toy corpus with window size $n=1$ (each word only \"sees\" its immediate left and right neighbors).\n",
        "\n",
        "| *        | all | ends | glitters | gold | is | not | that | well |\n",
        "|----------|-----|------|----------|------|----|-----|------|------|\n",
        "| all      | 0   | 0    | 0        | 0    | 1  | 0   | 1    | 0    |\n",
        "| ends     | 0   | 0    | 0        | 0    | 0  | 0   | 1    | 1    |\n",
        "| glitters | 0   | 0    | 0        | 0    | 1  | 0   | 1    | 0    |\n",
        "| gold     | 0   | 0    | 0        | 0    | 0  | 1   | 0    | 0    |\n",
        "| is       | 1   | 0    | 1        | 0    | 0  | 1   | 0    | 1    |\n",
        "| not      | 0   | 0    | 0        | 1    | 1  | 0   | 0    | 0    |\n",
        "| that     | 1   | 1    | 1        | 0    | 0  | 0   | 0    | 1    |\n",
        "| well     | 0   | 1    | 0        | 0    | 1  | 0   | 1    | 0    |\n",
        "\n",
        "### Distance Weighting\n",
        "\n",
        "Each row (or column) of $M$ can be viewed as a **count-based word vector**, but these vectors are typically very high-dimensional. A simple improvement is to use **distance weighting**: words that appear closer together in the window receive higher weights than those farther apart. We use the weight $w(d) = \\frac{1}{d}$, where $d$ is the distance between the center word and context word. This captures the intuition that immediate neighbors are more semantically relevant than distant ones.\n",
        "\n",
        "### PPMI Transform\n",
        "\n",
        "With large corpora, high-frequency words and common function words (like \"the\", \"and\") may dominate the raw co-occurrence counts, obscuring meaningful semantic relationships. **Positive Pointwise Mutual Information (PPMI)** addresses this by measuring whether two words co-occur more often than expected by chance:\n",
        "\n",
        "$$\n",
        "\\text{PMI}(w_i, w_j) = \\log \\frac{P(w_i, w_j)}{P(w_i) \\cdot P(w_j)}\n",
        "$$\n",
        "\n",
        "where $P(w_i, w_j)$ is the joint probability of the two words appearing together, and $P(w_i)$, $P(w_j)$ are their individual probabilities. PPMI takes the positive part: $\\text{PPMI}(w_i, w_j) = \\max(0, \\text{PMI}(w_i, w_j))$. This transformation reduces the influence of frequent but uninformative co-occurrences while preserving meaningful associations.\n",
        "\n",
        "### Dimensionality Reduction\n",
        "\n",
        "A common way to reduce dimensionality is **Singular Value Decomposition (SVD)** (closely related to PCA). In practice, we often keep only the top $k$ components (e.g., via truncated SVD) to obtain shorter vectors that capture the most important patterns in co-occurrence.\n",
        "\n",
        "![SVD Decomposition](imgs/svd.png)\n",
        "\n",
        "Projection doesn't create meaning—it can reveal structure that's already present in the data. After dimensionality reduction, words that appear in similar contexts (e.g., *doctor* and *hospital*) tend to be closer to each other than unrelated words (e.g., *doctor* and *dog*).\n",
        "\n",
        "If eigenvalues and SVD are new to you, [here](https://davetang.org/file/Singular_Value_Decomposition_Tutorial.pdf) is a beginner-friendly introduction. In practice, you will typically use library implementations (NumPy/SciPy/scikit-learn). On large corpora, computing a full SVD can be expensive, so we use **truncated** methods (like `TruncatedSVD`) to efficiently extract the top $k$ components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IKeK4xtSStv"
      },
      "source": [
        "### Plotting Co-Occurrence Word Embeddings\n",
        "\n",
        "In the next few cells, we’ll work with a small sample of **AG News** articles (**N_DOCS = 300**) to keep runtime reasonable. We construct a word–word co-occurrence matrix using a context window of **WINDOW_SIZE = 4**, and we discard infrequent words by keeping only those that appear at least **MIN_COUNT = 3** times.\n",
        "\n",
        "Preprocessing is intentionally minimal: all text is **lowercased**, and tokenization keeps **alphanumeric** tokens only. No additional preprocessing steps (such as stemming, lemmatization, or stopword removal) are required for this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wMfnjxG6uK8"
      },
      "outputs": [],
      "source": [
        "DATASET_NAME = 'ag_news'\n",
        "N_DOCS = 300\n",
        "WINDOW_SIZE = 4\n",
        "MIN_COUNT = 3\n",
        "\n",
        "\n",
        "def simple_tokenize(text):\n",
        "    '''Lowercase + keep alphanumerics; return list of tokens.'''\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
        "    tokens = [t for t in text.split() if t]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU6AlSt96uK8"
      },
      "source": [
        "We provide a helper function `load_corpus()` that loads a small, reproducible sample of **AG News** articles and returns them as tokenized documents.\n",
        "\n",
        "You do **not** need to apply any additional preprocessing for this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-03-27T08:06:28.097673Z",
          "start_time": "2024-03-27T08:06:28.094138Z"
        },
        "id": "xwD2htUoSStw"
      },
      "outputs": [],
      "source": [
        "def load_corpus(dataset_name=DATASET_NAME, n_docs=N_DOCS, seed=0):\n",
        "    '''Load and tokenize a small corpus for this assignment.'''\n",
        "    ds = load_dataset(dataset_name, split='train')\n",
        "    rng = random.Random(seed)\n",
        "    indices = list(range(len(ds)))\n",
        "    rng.shuffle(indices)\n",
        "    indices = indices[:n_docs]\n",
        "    docs = []\n",
        "    for i in indices:\n",
        "        text = ds[i]['text']\n",
        "        docs.append(simple_tokenize(text))\n",
        "    return docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVLquFhjSStx"
      },
      "source": [
        "Let’s inspect a few processed reviews to see what the tokenized documents look like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-03-27T08:06:29.881790Z",
          "start_time": "2024-03-27T08:06:29.404708Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361,
          "referenced_widgets": [
            "8edd7d55e30f465bb808b6776a44c22f",
            "e82f4c84d9b14d0c865b75e5d6fca012",
            "6676d23c869c4c3ab62687963c9c7810",
            "59b2fd5f89414776a0e0a3fd34c722ac",
            "a5de794946254027a0d49afeb0892ec4",
            "9cc5c22d97804947a1327190e5913e97",
            "8ea7f450ad7940c49c7e1579b1eef6c7",
            "a31c71fd829348feb18caa4bc63f4040",
            "efbe83768ae34b1db83fa6cad104d45f",
            "40382282dc4943ba8effdc629706b58f",
            "4f51ffd0cd394ecdbebffc9c1d2ef4d0",
            "f66bda42670e498f96c415b054103d76",
            "f8fa6ae187514f2393789cbc0e31d8f4",
            "c183984746ce46f3be1b7db347f529d5",
            "b4d23888bfe24de3bd7b17b27a913ba2",
            "05e6bc29a6024c26998a8d38aa876108",
            "8d29c9ba72b8475080a8a627cad79200",
            "1f0ccb8f46004bdf990b5e76ada71343",
            "d5e801c7db464bb98fed29d29a5dcd96",
            "7e9d529fe1de4ff2a89882f256909c24",
            "58d0c5456df84eee90ceda260c33b1d6",
            "cf0c230e554e47678d0743c3a2665f66",
            "df80555aca904fc78569a6d9e6c012c9",
            "9af480a284dd4450a2aff2f4a2b9c01c",
            "49e006e7e7b64b9790d1186bd7a2dbf3",
            "d12bb1f2387b4a54b8c284e926eba4d5",
            "57550049c5414e069f850aa457ea9b83",
            "fd7fb66483a94eb6807450fa0913c9fd",
            "987f8295f041452da4d749a55ce2c08e",
            "39180266a9594c238a41b9fbfc8c5204",
            "4a81af7c89674d4e9ce254bcc587bc13",
            "448db18dcb994dba838c83095efc1594",
            "283b05d2e26140338cf7843ad986171d",
            "6f6c1a4b707747d49ebbc6a5d1eb3e99",
            "1f93dad37fd34852a7548ffd68dee957",
            "d7ca6a3eccd841088c009a5cd1662065",
            "b826516701c84219b7a527f3f68bc158",
            "e1e2ddeae66c49d5ac1a8614cafbc360",
            "8f1e7618846042f081ba5310042b54f7",
            "2552277ab03c48548c87b32a7b7710a4",
            "f6ae47ae3f9849c4a543fff3a56344b0",
            "7eef364a042d49118c9c0544820f2905",
            "7e90ed1a82d04743a16636422c4b30c5",
            "691df78c687140a8bbe9966657e599fb",
            "08925b560ffd45b498872d72c488e9f9",
            "8c4aecb573a14addb79bee4513b18307",
            "ac5a30e347b34835806781b3d12dd934",
            "200a12213c0842ad87b5a7c08fea91e0",
            "545f468c00714ec1b4fe641898178723",
            "13a014a245ac4f1a887c242cca0b6809",
            "6b5f9053c6434ef39b83f68713924c37",
            "ad6f2adea3dd4f32a238ff11cd1d5849",
            "8e74726e602f48589ad8bf01901475ff",
            "b5c17b8684d544349ccce585e6ee4109",
            "f840c1d14bcf43bba1d61cc165f4f04d"
          ]
        },
        "id": "mC7B9Cb-SSty",
        "outputId": "24df0556-fa20-42e9-9f93-d94dca992674"
      },
      "outputs": [],
      "source": [
        "corpus = load_corpus()\n",
        "print('num documents:', len(corpus))\n",
        "print('tokens in first document:', len(corpus[0]))\n",
        "print('first 40 tokens:', corpus[0][:40])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfa216H1SSt0"
      },
      "source": [
        "### Question 1.1: Build a vocabulary with a minimum count [code] (6 points)\n",
        "\n",
        "Write a function that builds a vocabulary from `corpus` (a list of token lists).\n",
        "\n",
        "Requirements:\n",
        "- Count token frequencies across all documents.\n",
        "- Keep tokens that appear at least `min_count` times.\n",
        "- Return `vocab` as a **sorted list** and `word2id` as a mapping token -> index.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjJABbVFSSt1"
      },
      "outputs": [],
      "source": [
        "def build_vocab(corpus, min_count=MIN_COUNT):\n",
        "    '''Return (vocab, word2id, counts) where vocab is sorted.'''\n",
        "    counts = {}\n",
        "\n",
        "    # ------------------\n",
        "    # TODO: This function is mostly implemented for you.\n",
        "    # You only need to fill in ONE missing line below.\n",
        "    # ------------------\n",
        "    for doc in corpus:\n",
        "        for tok in doc:\n",
        "            counts[tok] = counts.get(tok, 0) + 1\n",
        "\n",
        "    vocab = sorted([w for w, c in counts.items() if c >= min_count])\n",
        "\n",
        "    # TODO: Build the token -> index mapping for vocab (1 line).\n",
        "    word2id = None\n",
        "\n",
        "    return vocab, word2id, counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKfXBXySSSt3",
        "outputId": "1048148d-ef1e-4c90-ca3a-fc1d80829095"
      },
      "outputs": [],
      "source": [
        "# Sanity check (deterministic toy corpus)\n",
        "toy = [['a','b','a'], ['b','c'], ['a','c','c']]\n",
        "vocab_toy, word2id_toy, counts_toy = build_vocab(toy, min_count=2)\n",
        "assert vocab_toy == ['a', 'b', 'c']\n",
        "assert counts_toy['a'] == 3 and counts_toy['b'] == 2 and counts_toy['c'] == 3\n",
        "assert word2id_toy == {'a': 0, 'b': 1, 'c': 2}\n",
        "print('Passed toy vocab test.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymDFJn_lSSt5"
      },
      "source": [
        "### Question 1.2: Distance-weighted co-occurrence counts [code] (8 points)\n",
        "\n",
        "We define a context window of size `window_size`. For each center token, look at tokens within that window.\n",
        "Add a contribution to the co-occurrence count that is larger for closer words.\n",
        "\n",
        "Use the weight: weight(d) = 1/d where d = |i-j|.\n",
        "\n",
        "Requirements:\n",
        "\n",
        "- Only count words that are in `word2id`.\n",
        "- Return a **square** NumPy array `C` of shape `(V, V)`.\n",
        "- Make `C` symmetric by adding both directions (center->context and context->center)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8MIy3KDSSt6"
      },
      "outputs": [],
      "source": [
        "def weighted_cooccurrence(corpus, word2id, window_size=WINDOW_SIZE):\n",
        "    '''Return symmetric weighted co-occurrence matrix C (float64).'''\n",
        "    V = len(word2id)\n",
        "    C = np.zeros((V, V), dtype=np.float64)\n",
        "\n",
        "    # ------------------\n",
        "    # TODO: The loops and indexing are provided.\n",
        "    # You only need to add the distance-weighted update (2-3 lines).\n",
        "    # Use weight(d) = 1/d where d = |i-j|.\n",
        "    # ------------------\n",
        "    for doc in corpus:\n",
        "        n = len(doc)\n",
        "        for i in range(n):\n",
        "            wi = doc[i]\n",
        "            if wi not in word2id:\n",
        "                continue\n",
        "            ui = word2id[wi]\n",
        "            j_max = min(n, i + window_size + 1)\n",
        "            for j in range(i + 1, j_max):\n",
        "                wj = doc[j]\n",
        "                if wj not in word2id:\n",
        "                    continue\n",
        "                vj = word2id[wj]\n",
        "\n",
        "                # TODO: compute distance d, weight w=1/d, and update BOTH C[ui,vj] and C[vj,ui].\n",
        "                # d = ...\n",
        "                # w = ...\n",
        "                # C[ui, vj] += w\n",
        "                # C[vj, ui] += w\n",
        "                raise NotImplementedError()\n",
        "\n",
        "    return C\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-6ID1qhSSt7",
        "outputId": "c93d2439-b2be-4044-b084-e0e65f5e4966"
      },
      "outputs": [],
      "source": [
        "# Small checks for weighted co-occurrence\n",
        "toy = [['a','b','c','b']]\n",
        "vocab_toy, word2id_toy, _ = build_vocab(toy, min_count=1)\n",
        "C = weighted_cooccurrence(toy, word2id_toy, window_size=2)\n",
        "assert C.shape == (3,3)\n",
        "assert np.allclose(C, C.T)\n",
        "print('Passed basic co-occurrence checks.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_4kvs0y6uLA"
      },
      "source": [
        "### Question 1.3: PPMI transform [code] (6 points)\n",
        "\n",
        "Raw co-occurrence counts are dominated by very frequent tokens. A common fix is to convert counts into\n",
        "PPMI (positive pointwise mutual information).\n",
        "\n",
        "Implement `ppmi(C)` to return a matrix of the same shape. Use a small epsilon to avoid divide-by-zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VghoRFHL6uLA"
      },
      "outputs": [],
      "source": [
        "def ppmi(C, eps=1e-12):\n",
        "    '''Convert co-occurrence matrix C to PPMI matrix.'''\n",
        "    # ------------------\n",
        "    # TODO: Most of this is implemented for you.\n",
        "    # You only need to fill in the PMI->PPMI step (2 lines).\n",
        "    # ------------------\n",
        "    C = np.asarray(C, dtype=np.float64)\n",
        "    total = float(C.sum())\n",
        "    if total <= 0:\n",
        "        return np.zeros_like(C)\n",
        "\n",
        "    row_sums = C.sum(axis=1, keepdims=True)\n",
        "    col_sums = C.sum(axis=0, keepdims=True)\n",
        "\n",
        "    p_uv = C / total\n",
        "    p_u = row_sums / total\n",
        "    p_v = col_sums / total\n",
        "\n",
        "    denom = p_u @ p_v\n",
        "\n",
        "    # TODO: compute PMI = log((p_uv + eps) / (denom + eps))\n",
        "    # TODO: return PPMI = max(PMI, 0) (elementwise)\n",
        "    raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNrWqrDs6uLB",
        "outputId": "8a1d93e6-db0b-45a9-d79f-f28367f0c00e"
      },
      "outputs": [],
      "source": [
        "# PPMI sanity check: output nonnegative and same shape\n",
        "C_small = np.array([[0., 1.], [1., 0.]])\n",
        "P_small = ppmi(C_small)\n",
        "assert P_small.shape == (2,2)\n",
        "assert np.all(P_small >= 0)\n",
        "print('Passed basic PPMI checks.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-nyJnAASSt9"
      },
      "source": [
        "### Question 1.4: Reduce and plot a 2D embedding [code] (6 points)\n",
        "\n",
        "In this question, you will implement the dimensionality-reduction function `reduce_2d` yourself.\n",
        "Use `TruncatedSVD` to reduce your PPMI matrix to 2D, then plot selected words with labels.\n",
        "\n",
        "Requirements:\n",
        "\n",
        "- `reduce_2d` must create a `TruncatedSVD` model with `n_components=2` and return an array of shape `(V, 2)` (or more generally `(n_rows, 2)`).\n",
        "- You should use `fit_transform` (or `fit` + `transform`) rather than computing SVD manually.\n",
        "- `plot_words_2d` plots only the requested words (skip words not in the vocab).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGOkGqpH6uLB"
      },
      "outputs": [],
      "source": [
        "def reduce_2d(X, n_iter=10, random_state=0):\n",
        "    '''Return 2D embedding from matrix X using truncated SVD.'''\n",
        "    # ------------------\n",
        "    # TODO: Implement this function.\n",
        "    #\n",
        "    # Requirements:\n",
        "    # - Use TruncatedSVD from sklearn.decomposition (already imported).\n",
        "    # - Use n_components=2, and pass through n_iter and random_state.\n",
        "    # - Return Z with shape (X.shape[0], 2).\n",
        "    #\n",
        "    # Hint: svd.fit_transform(X) returns the reduced representation.\n",
        "    # ------------------\n",
        "    raise NotImplementedError()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "truGMjifSSt9"
      },
      "outputs": [],
      "source": [
        "def plot_words_2d(Z, word2id, words, title=None):\n",
        "    '''Scatter plot for selected words; skip missing words.'''\n",
        "    # ------------------\n",
        "    # TODO: Only fill in the missing parts in the loop below.\n",
        "    # ------------------\n",
        "    xs = []\n",
        "    ys = []\n",
        "    labels = []\n",
        "\n",
        "    for w in words:\n",
        "        idx = word2id.get(w)\n",
        "        if idx is None:\n",
        "            continue\n",
        "\n",
        "        # TODO: append x and y coordinates from Z, and the label w\n",
        "        \n",
        "        raise NotImplementedError()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.scatter(xs, ys, alpha=0.8)\n",
        "    for x, y, w in zip(xs, ys, labels):\n",
        "        plt.annotate(w, (x, y), textcoords=\"offset points\", xytext=(4, 2), ha=\"left\")\n",
        "\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.xlabel('component 1')\n",
        "    plt.ylabel('component 2')\n",
        "    plt.grid(True, linestyle='--', alpha=0.3)\n",
        "    plt.show()\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "gHxOMWPxSSuB",
        "outputId": "27e7052d-8dc1-4f43-a59d-bee89eaaa819"
      },
      "outputs": [],
      "source": [
        "# ---------------------\n",
        "# Run this sanity check\n",
        "# Note that this is not an exhaustive check for correctness.\n",
        "# The plot produced should look like the included file question_1.4_test.png\n",
        "# ---------------------\n",
        "\n",
        "print (\"-\" * 80)\n",
        "print (\"Outputted Plot:\")\n",
        "\n",
        "M_reduced_plot_test = np.array([[1, 1], [-1, -1], [1, -1], [-1, 1], [0, 0]])\n",
        "word2ind_plot_test = {'test1': 0, 'test2': 1, 'test3': 2, 'test4': 3, 'test5': 4}\n",
        "words = ['test1', 'test2', 'test3', 'test4', 'test5']\n",
        "plot_words_2d(M_reduced_plot_test, word2ind_plot_test, words)\n",
        "\n",
        "print (\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpBzYs2hSSuC"
      },
      "source": [
        "### Question 1.5: Co-occurrence embedding analysis [written] (4 points)\n",
        "\n",
        "**Run the cell below to generate the plot.** It may take a few minutes to compute the co-occurrence matrix and run SVD (depending on your machine).\n",
        "After the plot is generated, answer the interpretation prompt in the next markdown cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "7L1Uk50mSSuD",
        "outputId": "e57c61dc-f550-4dc6-c7ce-477fa31217de"
      },
      "outputs": [],
      "source": [
        "# Build Part 1 embeddings (this may take ~1-3 minutes depending on your machine)\n",
        "vocab, word2id, counts = build_vocab(corpus, min_count=MIN_COUNT)\n",
        "print('vocab size:', len(vocab))\n",
        "C = weighted_cooccurrence(corpus, word2id, window_size=WINDOW_SIZE)\n",
        "P = ppmi(C)\n",
        "Z2 = reduce_2d(P)\n",
        "\n",
        "words_to_plot = ['market', 'stocks', 'game', 'team', 'war', 'election', 'police', 'technology', 'internet', 'health']\n",
        "plot_words_2d(Z2, word2id, words_to_plot, title='PPMI + TruncatedSVD (2D)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFisRSwn6uLC"
      },
      "source": [
        "#### Interpretation\n",
        "\n",
        "Look at your Part 1 plot. In 4-6 sentences:\n",
        "- describe one cluster you see (which words are near each other?),\n",
        "- propose a reason based on news topics/contexts,\n",
        "- name one surprising placement and give a plausible explanation.\n",
        "\n",
        "#### <font color=\"red\">Write your answer here.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h0OzAaRSSuI"
      },
      "source": [
        "## Part 2: Prediction-Based Word Vectors (65 points)\n",
        "\n",
        "In Part 1 you built **count-based** word vectors from a co-occurrence matrix. In Part 2, we’ll look at **prediction-based** embeddings—vectors learned by optimizing an objective function on large text corpora. Two classic examples are **word2vec** and **GloVe**.\n",
        "\n",
        "In this assignment we will explore **pretrained GloVe embeddings**. GloVe uses global co-occurrence statistics during training, but the final vectors are learned (not raw counts). You do **not** need to train GloVe yourself—we will load a pretrained **100-dimensional** GloVe model using `gensim`.\n",
        "\n",
        "**Runtime notes:**\n",
        "\n",
        "- The first time you run the loading cell, it will download the model and may take a few minutes.\n",
        "- If you have downloaded it before, rerunning the cell will load from the local cache and should be faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3kwvdksSSuI",
        "outputId": "c7f2070f-9a4b-486d-e10f-893231634c3b"
      },
      "outputs": [],
      "source": [
        "def load_embedding_model():\n",
        "    \"\"\" Load GloVe Vectors\n",
        "        Return:\n",
        "            wv_from_bin: All 400000 embeddings, each length 100\n",
        "    \"\"\"\n",
        "    import gensim.downloader as api\n",
        "    wv_from_bin = api.load(\"glove-wiki-gigaword-100\")\n",
        "    print(\"Loaded vocab size %i\" % len(list(wv_from_bin.index_to_key)))\n",
        "    return wv_from_bin\n",
        "wv_from_bin = load_embedding_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egIeG1RTSSuK"
      },
      "source": [
        "#### Note: If you are receiving a \"reset by peer\" error, rerun the cell to restart the download."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH9gWJWpSSuL"
      },
      "source": [
        "### Reducing dimensionality of Word Embeddings\n",
        "\n",
        "Let's directly compare the GloVe embeddings to those of the co-occurrence matrix. In order to avoid running out of memory, we will work with a sample of 40000 GloVe vectors instead.\n",
        "Run the following cells to:\n",
        "\n",
        "1. Put 40000 Glove vectors into a matrix M\n",
        "2. Run `reduce_2d` (your Truncated SVD function) to reduce the vectors from 100-dimensional to 2-dimensional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PROUu_-SSuL"
      },
      "outputs": [],
      "source": [
        "def get_matrix_of_vectors(wv_from_bin, required_words):\n",
        "    \"\"\" Put the GloVe vectors into a matrix M.\n",
        "        Param:\n",
        "            wv_from_bin: KeyedVectors object; the 400000 GloVe vectors loaded from file\n",
        "        Return:\n",
        "            M: numpy matrix shape (num words, 100) containing the vectors\n",
        "            word2ind: dictionary mapping each word to its row number in M\n",
        "    \"\"\"\n",
        "    import random\n",
        "    words = list(wv_from_bin.index_to_key)\n",
        "    print(\"Shuffling words ...\")\n",
        "    random.seed(225)\n",
        "    random.shuffle(words)\n",
        "    print(\"Putting %i words into word2ind and matrix M...\" % len(words))\n",
        "    word2ind = {}\n",
        "    M = []\n",
        "    curInd = 0\n",
        "    for w in words:\n",
        "        try:\n",
        "            M.append(wv_from_bin.get_vector(w))\n",
        "            word2ind[w] = curInd\n",
        "            curInd += 1\n",
        "        except KeyError:\n",
        "            continue\n",
        "    for w in required_words:\n",
        "        if w in words:\n",
        "            continue\n",
        "        try:\n",
        "            M.append(wv_from_bin.get_vector(w))\n",
        "            word2ind[w] = curInd\n",
        "            curInd += 1\n",
        "        except KeyError:\n",
        "            continue\n",
        "    M = np.stack(M)\n",
        "    print(\"Done.\")\n",
        "    return M, word2ind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpgM0M-hSSuM",
        "outputId": "4e272acd-33f6-4a93-9ea4-d3e21c1e5f78"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------------\n",
        "# Run Cell to Reduce 100-Dimensional Word Embeddings to k Dimensions\n",
        "# Note: This should be quick to run\n",
        "# -----------------------------------------------------------------\n",
        "M, word2ind = get_matrix_of_vectors(wv_from_bin, words_to_plot)\n",
        "M_reduced = reduce_2d(M, n_iter=10, random_state=0)\n",
        "\n",
        "# Rescale (normalize) the rows to make them each of unit-length\n",
        "M_lengths = np.linalg.norm(M_reduced, axis=1)\n",
        "M_reduced_normalized = M_reduced / M_lengths[:, np.newaxis] # broadcasting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_46FYMJSSuN"
      },
      "source": [
        "**Note: If you are receiving out of memory issues on your local machine, try closing other applications to free more memory on your device. You may want to try restarting your machine so that you can free up extra memory. Then immediately run the jupyter notebook and see if you can load the word vectors properly. If you still have problems with loading the embeddings onto your local machine after this, please go to office hours or contact course staff.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAFrU8ahSSuO"
      },
      "source": [
        "### Question 2.1: GloVe plot analysis [written] (7 points)\n",
        "\n",
        "Run the cell below to plot the 2D GloVe embeddings for `['movie', 'book', 'mysterious', 'story', 'fascinating', 'good', 'interesting', 'large', 'massive', 'huge']`. Then answer:\n",
        "\n",
        "a. Describe one direction or axis you observe (what changes along it?).\n",
        "b. Name one word that seems out of place and propose a reason.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "9kHRkjz6SSuO",
        "outputId": "b716ecd3-b01e-4914-a7a9-82bdee0a7340",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# plot the same words as question 1.5\n",
        "plot_words_2d(M_reduced_normalized, word2ind, words_to_plot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOnrMZkzSSuP"
      },
      "source": [
        "a. What is one way the plot is different from the one generated earlier from the co-occurrence matrix? What is one way it's similar?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KC4PTQoSSuQ"
      },
      "source": [
        "#### <font color=\"red\">Write your answer here.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNDY5puZSSuQ"
      },
      "source": [
        "b. Why might the GloVe plot (question_2.1.png) differ from the plot generated earlier from the co-occurrence matrix (question_1.5.png)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-cWAvi8SSuR"
      },
      "source": [
        "#### <font color=\"red\">Write your answer here.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA8oIbjjSSuS"
      },
      "source": [
        "### Cosine Similarity\n",
        "\n",
        "Now that we have word vectors, we need a way to quantify how **similar** two words are according to their vectors. A standard choice is **cosine similarity**, which compares the *angle* between two vectors (so it focuses on direction rather than raw length). We’ll use it throughout Part 2 to find words that are “close” or “far” in the pretrained GloVe space.\n",
        "\n",
        "You can think of $n$-dimensional vectors as points in $\\mathbb{R}^n$. Distances such as [L1](http://mathworld.wolfram.com/L1-Norm.html) and [L2](http://mathworld.wolfram.com/L2-Norm.html) measure how far apart points are, but cosine similarity instead measures how aligned the vectors are:\n",
        "\n",
        "<img src=\"./imgs/inner_product.png\" width=20% style=\"float: center;\"></img>\n",
        "\n",
        "Instead of computing the angle $\\Theta$ explicitly, we use $\\cos(\\Theta)$. Formally, the [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) $s$ between two vectors $p$ and $q$ is:\n",
        "\n",
        "$$\n",
        "s = \\frac{p \\cdot q}{\\lVert p \\rVert\\, \\lVert q \\rVert}, \\qquad s \\in [-1, 1]\n",
        "$$\n",
        "\n",
        "- $s \\approx 1$: very similar directions (often semantically related).\n",
        "- $s \\approx 0$: roughly unrelated directions.\n",
        "- $s \\approx -1$: opposite directions (not necessarily antonyms in practice).\n",
        "\n",
        "**Cosine distance** is defined as $1 - s$ and is what `wv_from_bin.distance(w1, w2)` returns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFfCOLUsSSuS"
      },
      "source": [
        "### Question 2.2: Words with multiple senses [code + written] (6 points)\n",
        "Polysemes and homonyms have more than one meaning. Find a word with *at least two different senses* such that the top-10 most similar words (by cosine similarity) include related words from *both* senses. You will likely need to try several candidates.\n",
        "\n",
        "State the word you found and the two meanings reflected in the top 10. Why do many candidate words fail (i.e., the top-10 list reflects only one sense)?\n",
        "\n",
        "**Note**: Use `wv_from_bin.most_similar(word)` to get the top 10 most similar words. For details, see the [GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.most_similar).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAr09U-xSSuT"
      },
      "outputs": [],
      "source": [
        "# ------------------\n",
        "# Write your implementation here.\n",
        "#\n",
        "# Tip: You will probably need to try multiple candidate words.\n",
        "# The helper below makes it easier to inspect nearest neighbors.\n",
        "# ------------------\n",
        "\n",
        "def top_similar(model, word, topn=10):\n",
        "    \"\"\"Return and print top-n most similar words (cosine similarity).\"\"\"\n",
        "    try:\n",
        "        sims = model.most_similar(word, topn=topn)\n",
        "    except KeyError:\n",
        "        print(f\"'{word}' is not in the vocabulary.\")\n",
        "        return []\n",
        "    print(f\"Top {topn} most similar to '{word}':\")\n",
        "    for w, s in sims:\n",
        "        print(f\"  {w:20s} {s:.4f}\")\n",
        "    return sims\n",
        "\n",
        "# Example usage (replace with your own search):\n",
        "# candidates = ['bank', 'bat', 'spring']  # try a few polysemous words\n",
        "# for c in candidates:\n",
        "#     print('-' * 60)\n",
        "#     top_similar(wv_from_bin, c)\n",
        "\n",
        "# ------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdQ018tjSSuT"
      },
      "source": [
        "#### <font color=\"red\">Write your answer here.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfeW-eK9SSuU"
      },
      "source": [
        "### Question 2.3: Synonyms vs. antonyms [code + written] (4 points)\n",
        "\n",
        "Cosine Distance is defined as 1 - Cosine Similarity.\n",
        "\n",
        "Find three words $(w_1,w_2,w_3)$ where $w_1$ and $w_2$ are synonyms and $w_1$ and $w_3$ are antonyms, but Cosine Distance $(w_1,w_3) <$ Cosine Distance $(w_1,w_2)$.\n",
        "\n",
        "Example (do not reuse): $w_1$=\"fast\", $w_2$=\"quick\", $w_3$=\"slow\" might show this effect. Provide your own example and a short explanation for why the counter-intuitive result can happen.\n",
        "\n",
        "Use `wv_from_bin.distance(w1, w2)` to compute cosine distance. See the [GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.distance).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwlpPjpHSSuV"
      },
      "outputs": [],
      "source": [
        "# ------------------\n",
        "# Write your implementation here.\n",
        "#\n",
        "# You want synonyms (w1, w2) and an antonym (w1, w3) such that:\n",
        "#   distance(w1, w3) < distance(w1, w2)\n",
        "# where distance = 1 - cosine_similarity.\n",
        "# ------------------\n",
        "\n",
        "def show_cosine_distances(model, w1, w2, w3):\n",
        "    \"\"\"Print cosine distances for (w1,w2) and (w1,w3).\"\"\"\n",
        "    try:\n",
        "        d12 = float(model.distance(w1, w2))\n",
        "        d13 = float(model.distance(w1, w3))\n",
        "    except KeyError as e:\n",
        "        print(f\"Word not in vocabulary: {e}\")\n",
        "        return None\n",
        "    print(f\"d({w1}, {w2}) = {d12:.4f}\")\n",
        "    print(f\"d({w1}, {w3}) = {d13:.4f}\")\n",
        "    print(\"Condition d(w1,w3) < d(w1,w2):\", d13 < d12)\n",
        "    return d12, d13\n",
        "\n",
        "# Example usage (replace with your own words):\n",
        "# w1, w2, w3 = 'fast', 'quick', 'slow'\n",
        "# show_cosine_distances(wv_from_bin, w1, w2, w3)\n",
        "\n",
        "# ------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeIHjTFMSSuV"
      },
      "source": [
        "#### <font color=\"red\">Write your answer here.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxIDq26zSSuW"
      },
      "source": [
        "### Question 2.4: Analogies with word vectors [written] (6 points)\n",
        "\n",
        "Word vectors can sometimes solve analogies using **vector arithmetic**.\n",
        "\n",
        "In this question, we will use the classic analogy form:\n",
        "\n",
        "> $x : y \\;::\\; a : b$\n",
        "\n",
        "Intuitively, we are asking for a word $b$ such that the relationship “from $x$ to $y$” is similar to the relationship “from $a$ to $b$”. In embedding space, this is often modeled as:\n",
        "\n",
        "$$\n",
        "\\text{vector}(b) \\approx \\text{vector}(y) - \\text{vector}(x) + \\text{vector}(a).\n",
        "$$\n",
        "\n",
        "Run the cell below to see the model’s top candidates for the analogy **man : grandfather :: woman : ?**.\n",
        "\n",
        "**How `most_similar` is used:** `most_similar(positive=[...], negative=[...])` returns words that are close to the sum of the positive vectors and far from the sum of the negative vectors (by cosine similarity)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0pC7H4VSSuY",
        "outputId": "7f22f256-8d60-4a4e-acf3-11b8d09970f3"
      },
      "outputs": [],
      "source": [
        "# Run this cell to answer the analogy: man : grandfather :: woman : x\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'grandfather'], negative=['man']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVv8I9WwSSuZ"
      },
      "source": [
        "Let $m$, $g$, $w$, and $x$ denote the word vectors for `man`, `grandfather`, `woman`, and the (unknown) answer, respectively.\n",
        "\n",
        "The call below searches over candidate words $x$ and returns those with the **highest cosine similarity** to a *target vector* formed by adding the `positive` vectors and subtracting the `negative` vectors:\n",
        "\n",
        "```python\n",
        "wv_from_bin.most_similar(positive=['woman', 'grandfather'], negative=['man'])\n",
        "```\n",
        "\n",
        "Using **only** $m$, $g$, $w$ and the operators $+$ and $-$, write the target vector that candidates are compared to. Then state the cosine-similarity objective in symbols (e.g., “maximize $\\cos(\\cdot,\\cdot)$ …”).\n",
        "\n",
        "Hint: The target vector is $w + g - m$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlUKBqtHSSuZ"
      },
      "source": [
        "#### <font color=\"red\">Write your answer here.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rRgMca9SSua"
      },
      "source": [
        "### Question 2.5: Interpreting analogy results [code + written] (7 points)\n",
        "\n",
        "**(a) (2 points)** In Question 2.4, `most_similar` often returns several *related* words (e.g., “granddaughter”, “daughter”, “mother”), not just one “perfect” answer.\n",
        "\n",
        "In 2–4 sentences, explain **why multiple answers can be reasonable**. Your explanation should reference at least one of the following ideas:\n",
        "- the analogy query produces a *target vector* and returns the nearest neighbors to that vector (not a single “proof”),\n",
        "- several words may be similarly close in cosine similarity,\n",
        "- the relationship you are asking for (e.g., “female counterpart of a family role”) is not uniquely represented by one word in the embedding space,\n",
        "- vocabulary / corpus effects (what the model saw during training)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgYQXazQSSua"
      },
      "source": [
        "#### <font color=\"red\">Write your answer here.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9aAUXEISSub"
      },
      "source": [
        "**(b) (5 points)** Find an analogy that **does** hold for these vectors (i.e., the intended word is ranked #1).\n",
        "\n",
        "In your solution, state the full analogy in the form **x : y :: a : b** and briefly explain (1–2 sentences) why it makes sense.\n",
        "\n",
        "**Tips:**\n",
        "- You may need to try several analogies before you find one that works.\n",
        "- Pick words that are common in Wikipedia/news-style text (the kind of data GloVe was trained on).\n",
        "- You can use the helper in the next cell to print the top candidates and verify that your intended $b$ is ranked #1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "CRvYK2xifpq7",
        "outputId": "ac3f7104-f085-4e7d-f51b-8bba01a798c5"
      },
      "outputs": [],
      "source": [
        "# Choose an analogy x:y :: a:b that the embeddings solve correctly (b is ranked #1).\n",
        "# For example: x, y, a, b = (\"\", \"\", \"\", \"\")\n",
        "\n",
        "def check_analogy(model, x, y, a, b, topn=10):\n",
        "    \"\"\"Print the top candidates for y - x + a and check if b is ranked #1.\"\"\"\n",
        "    res = model.most_similar(positive=[a, y], negative=[x], topn=topn)\n",
        "    print(f\"Analogy: {x}:{y} :: {a}:{b}\")\n",
        "    print(\"Top candidates:\")\n",
        "    for w, s in res:\n",
        "        print(f\"  {w:20s} {s:.4f}\")\n",
        "    print(\"Top-1 prediction:\", res[0][0])\n",
        "    return res\n",
        "\n",
        "# ------------------\n",
        "# Write your implementation here.\n",
        "# 1) Set x, y, a, b to define your analogy.\n",
        "# 2) (Optional) Call check_analogy(...) to inspect the top candidates.\n",
        "# ------------------\n",
        "\n",
        "# x, y, a, b = (\"\", \"\", \"\", \"\")\n",
        "\n",
        "# Optional sanity check before the assert:\n",
        "# _ = check_analogy(wv_from_bin, x, y, a, b, topn=10)\n",
        "\n",
        "# ------------------\n",
        "\n",
        "# Test the solution (b must be ranked #1)\n",
        "assert wv_from_bin.most_similar(positive=[a, y], negative=[x])[0][0] == b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3QlPqAwSSub"
      },
      "source": [
        "#### <font color=\"red\">Write your answer here.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwgcEywwSSuc"
      },
      "source": [
        "### Question 2.6: When analogies go wrong [code + written] (7 points)\n",
        "\n",
        "Analogy solving with embeddings is *suggestive*, not guaranteed. The vector arithmetic $y - x + a$ can fail when relationships are not represented cleanly in the training data or when a word has multiple strong associations.\n",
        "\n",
        "**(a) (2 points)** Below, we try to solve the analogy **hand : glove :: foot : sock** using:\n",
        "$$\n",
        "\\text{sock} \\;\\approx\\; \\text{foot} - \\text{hand} + \\text{glove}.\n",
        "$$\n",
        "\n",
        "Run the cell and look at the top candidates. In 2–4 sentences, give a plausible reason the intended answer might not be ranked #1. Your explanation should reference at least one of these ideas:\n",
        "- the training corpus may not strongly encode the “wears/used-with” relation in a consistent direction,\n",
        "- words like *glove* and *sock* have other associations (sports, materials, idioms, etc.),\n",
        "- analogies work best for certain relation types (e.g., country–capital, singular–plural),\n",
        "- noise from nearest-neighbor search / multiple near-ties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-ykWoJoSSuc",
        "outputId": "e7599e31-3cbf-4809-aeb8-b5ff6afc9d83"
      },
      "outputs": [],
      "source": [
        "# Analogy query: hand : glove :: foot : ?\n",
        "# i.e., glove - hand + foot\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['foot', 'glove'], negative=['hand']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zn4ruS8MSSud"
      },
      "source": [
        "#### <font color=\"red\">Write your answer here.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1gHyZt0SSud"
      },
      "source": [
        "**(b) (5 points)** Find another analogy that does **not** hold according to these vectors.\n",
        "\n",
        "In your solution:\n",
        "\n",
        "- State the *intended* analogy in the form **x : y :: a : b** (where $b$ is what a human would expect).\n",
        "- Compute the model’s top-1 prediction $\\hat{b}$ for the query `most_similar(positive=[a, y], negative=[x])`.\n",
        "- Report the incorrect prediction $\\hat{b}$ (and optionally the top-5 list) and briefly explain (1–2 sentences) why the model might prefer $\\hat{b}$ over $b$.\n",
        "\n",
        "Tip: Choose a relationship that is subtle, culturally specific, or not consistently expressed in text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ms-DTC8_ftiA"
      },
      "outputs": [],
      "source": [
        "# Find an analogy x:y :: a:b that the embeddings get wrong (b is NOT ranked #1).\n",
        "# For example: x, y, a, b = (\"\", \"\", \"\", \"\")\n",
        "\n",
        "def show_analogy_results(model, x, y, a, topn=10):\n",
        "    \"\"\"Print top-n candidates for y - x + a and return the result list.\"\"\"\n",
        "    res = model.most_similar(positive=[a, y], negative=[x], topn=topn)\n",
        "    print(f\"Analogy query: {x}:{y} :: {a}: ?\")\n",
        "    for w, s in res:\n",
        "        print(f\"  {w:20s} {s:.4f}\")\n",
        "    return res\n",
        "\n",
        "# ------------------\n",
        "# Write your implementation here.\n",
        "# 1) Set x, y, a, b for your intended analogy x:y :: a:b.\n",
        "# 2) Call show_analogy_results(...) and store the model's top-1 prediction in b_hat.\n",
        "# ------------------\n",
        "\n",
        "# x, y, a, b = (\"\", \"\", \"\", \"\")\n",
        "\n",
        "# res = show_analogy_results(wv_from_bin, x, y, a, topn=10)\n",
        "# b_hat = res[0][0]  # model's (incorrect) prediction\n",
        "\n",
        "# ------------------\n",
        "# Print the full ranked list (provided starter behavior)\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=[a, y], negative=[x]))\n",
        "\n",
        "# Test: the model should NOT rank your intended b at #1\n",
        "assert wv_from_bin.most_similar(positive=[a, y], negative=[x])[0][0] != b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4x0EHjeSSue"
      },
      "source": [
        "#### <font color=\"red\">Write your answer here.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvlycXN-SSuf"
      },
      "source": [
        "### Question 2.7: Guided analysis of bias in word vectors [written] (6 points)\n",
        "\n",
        "Word embeddings can encode **societal biases** present in their training data. This question asks you to *observe and describe* patterns in the pretrained vectors—not to endorse them.\n",
        "\n",
        "Run the cell below. It produces two lists of nearest neighbors for analogy-style queries that differ only by swapping `man` and `woman`:\n",
        "\n",
        "- Query A: similar to `man` and `profession`, dissimilar to `woman`\n",
        "- Query B: similar to `woman` and `profession`, dissimilar to `man`\n",
        "\n",
        "In **4–6 sentences**, compare the two lists and answer:\n",
        "\n",
        "- What differences do you notice in the types of occupations/terms suggested?\n",
        "- What do those differences suggest about the associations the embeddings have learned?\n",
        "- Give **one** plausible explanation for where such bias could come from (think training data + objective).\n",
        "\n",
        "Note: Individual terms may be noisy. Focus on overall patterns (e.g., recurring themes or categories)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XggWA4MhSSuf",
        "outputId": "f156fbf4-3c1d-40a1-fd24-9fb363a63f33"
      },
      "outputs": [],
      "source": [
        "# Run this cell\n",
        "# Here `positive` indicates words we want to be similar to, and `negative` indicates words we want to be dissimilar from.\n",
        "# As you inspect the outputs, look for broad patterns (e.g., recurring occupations, stereotypes, or themes).\n",
        "\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['man', 'profession'], negative=['woman']))\n",
        "print()\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'profession'], negative=['man']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4g6KbsYSSuh"
      },
      "source": [
        "#### <font color=\"red\">Write your answer here.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxJmnS6lSSui"
      },
      "source": [
        "### Question 2.8: Independent analysis of bias in word vectors [code + written] (6 points)\n",
        "\n",
        "In Question 2.7 you analyzed a provided example. Here you will design your **own** query that reveals a biased association learned by the pretrained embeddings.\n",
        "\n",
        "**Task:** Use `wv_from_bin.most_similar(positive=[...], negative=[...])` to create a pair of analogy-style queries that are identical except for swapping one word (e.g., swapping a demographic term), and compare the results.\n",
        "\n",
        "In your submission:\n",
        "\n",
        "- Show the code you ran and the top results it produced (top-10 is fine).\n",
        "- In 3–5 sentences, describe the pattern you observe and explain why you consider it biased or stereotyped.\n",
        "- Give one plausible explanation for where the association could come from (training data and/or objective).\n",
        "\n",
        "Important: Describe the phenomenon at a high level. You do not need to reproduce or emphasize offensive language; focus on overall patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZoDheIfSSui"
      },
      "outputs": [],
      "source": [
        "# ------------------\n",
        "# Write your implementation here.\n",
        "#\n",
        "# Goal: find an analogy-style query that surfaces a biased association in the embeddings.\n",
        "# You are NOT expected to “fix” anything here—just observe and describe patterns.\n",
        "# ------------------\n",
        "\n",
        "def compare_analogy(model, positive_a, negative_a, positive_b, negative_b, topn=10):\n",
        "    \"\"\"Run two most_similar queries side-by-side for easier comparison.\"\"\"\n",
        "    res_a = model.most_similar(positive=positive_a, negative=negative_a, topn=topn)\n",
        "    res_b = model.most_similar(positive=positive_b, negative=negative_b, topn=topn)\n",
        "    print(\"Query A:\")\n",
        "    print(\"  positive =\", positive_a, \" negative =\", negative_a)\n",
        "    pprint.pprint(res_a)\n",
        "    print()\n",
        "    print(\"Query B:\")\n",
        "    print(\"  positive =\", positive_b, \" negative =\", negative_b)\n",
        "    pprint.pprint(res_b)\n",
        "    return res_a, res_b\n",
        "\n",
        "# ------------------\n",
        "# Suggested workflow:\n",
        "# 1) Pick two parallel queries that differ by swapping one word (e.g., swap one demographic term).\n",
        "# 2) Run compare_analogy(...) to print both top-10 lists.\n",
        "# 3) In the written response, describe the overall pattern (categories/themes), not individual slurs.\n",
        "# ------------------\n",
        "\n",
        "# Example template (fill in your own words):\n",
        "# res_a, res_b = compare_analogy(\n",
        "#     wv_from_bin,\n",
        "#     positive_a=[...], negative_a=[...],\n",
        "#     positive_b=[...], negative_b=[...]\n",
        "# )\n",
        "\n",
        "# ------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGOlmtJoSSuj"
      },
      "source": [
        "#### <font color=\"red\">Write your answer here.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK2XVWzmSSuk"
      },
      "source": [
        "### Question 2.9: Thinking about bias sources [written] (10 points)\n",
        "\n",
        "**(a) (5 points)** Give **one** plausible explanation for *how bias enters word vectors* during training.\n",
        "\n",
        "Focus on the **training data** and the **training objective** (how the model learns from co-occurrence / context). Avoid discussing downstream LLM behaviors.\n",
        "\n",
        "In 4–6 sentences, explain your reasoning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19pM85fCSSuk"
      },
      "source": [
        "#### <font color=\"red\">Write your answer here.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILYqJZ7ASSul"
      },
      "source": [
        "**(b) (5 points)** Describe **one** method to mitigate bias in word vectors in 4–6 sentences:\n",
        "\n",
        "Examples of acceptable directions (pick one; do not do all):\n",
        "\n",
        "- **Data curation / balancing**: adjust the training corpus to reduce biased co-occurrence patterns.\n",
        "- **Post-processing (debiasing)**: identify a “bias direction” and reduce its influence on embeddings while keeping other structure.\n",
        "- **Constraint/objective changes**: add regularization or constraints during training to limit biased associations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnJaAB7mSSul"
      },
      "source": [
        "\n",
        "#### <font color=\"red\">Write your answer here.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EL4C9mJ86uLV"
      },
      "source": [
        "## AI Use Disclosure (Required) (5 points)\n",
        "\n",
        "If you used any AI-enabled tools (e.g., ChatGPT, GitHub Copilot, Claude, or other LLM assistants) while working on this assignment, you must disclose that use here. The goal is transparency—not punishment.\n",
        "\n",
        "In your disclosure, briefly include:\n",
        "\n",
        "- **Tool(s) used:** (name + version if known)\n",
        "- **How you used them:** (e.g., concept explanation, debugging, drafting code, rewriting text)\n",
        "- **What you verified yourself:** (e.g., reran the notebook, checked outputs/plots, checked shapes, read documentation)\n",
        "- **What you did *not* use AI for (if applicable):** (optional)\n",
        "\n",
        "You are responsible for the correctness of your submission, even if AI suggested code or explanations.\n",
        "\n",
        "#### <font color=\"red\">Write your disclosure here.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzh3eEmZSSum"
      },
      "source": [
        "# <font color=\"blue\"> Submission Instructions</font>\n",
        "\n",
        "1. **Click the Save button** at the top of the Jupyter Notebook.\n",
        "\n",
        "2. **Click the \"Clear All Outputs\" icon** in the toolbar.   \n",
        "This will clear all outputs from all cells, but keep the content of the cells.\n",
        "\n",
        "3. **Click the \"Run All\" icon** in the toolbar.  \n",
        "This will run all cells in order; it may take several minutes.\n",
        "\n",
        "4. **Generate the HTML file using Quarto.**  \n",
        "Open a terminal, navigate to the directory containing your notebook (`.ipynb` file), and run ```quarto render filename.ipynb --to html```. If you have never used Quarto before, please follow the instructions [here](https://lizhen0909.github.io/Intro_to_programming_for_data_sci/venv_setup.html#rendering-notebook-as-html-using-quarto).\n",
        "\n",
        "5. **Review your HTML file.**  \n",
        "   Make sure all your solutions are included and displayed correctly — the HTML is the only thing your graders will see!\n",
        "6. **Submit your HTML file on Canvas.**\n",
        "7.  **Commit all your work to your GitHub repository.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K93oH-8z6uLW"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4ikVlJf6uLW"
      },
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05e6bc29a6024c26998a8d38aa876108": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08925b560ffd45b498872d72c488e9f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8c4aecb573a14addb79bee4513b18307",
              "IPY_MODEL_ac5a30e347b34835806781b3d12dd934",
              "IPY_MODEL_200a12213c0842ad87b5a7c08fea91e0"
            ],
            "layout": "IPY_MODEL_545f468c00714ec1b4fe641898178723"
          }
        },
        "13a014a245ac4f1a887c242cca0b6809": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f0ccb8f46004bdf990b5e76ada71343": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f93dad37fd34852a7548ffd68dee957": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f1e7618846042f081ba5310042b54f7",
            "placeholder": "​",
            "style": "IPY_MODEL_2552277ab03c48548c87b32a7b7710a4",
            "value": "Generating train split: 100%"
          }
        },
        "200a12213c0842ad87b5a7c08fea91e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5c17b8684d544349ccce585e6ee4109",
            "placeholder": "​",
            "style": "IPY_MODEL_f840c1d14bcf43bba1d61cc165f4f04d",
            "value": " 7600/7600 [00:00&lt;00:00, 179631.63 examples/s]"
          }
        },
        "2552277ab03c48548c87b32a7b7710a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "283b05d2e26140338cf7843ad986171d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39180266a9594c238a41b9fbfc8c5204": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40382282dc4943ba8effdc629706b58f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "448db18dcb994dba838c83095efc1594": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49e006e7e7b64b9790d1186bd7a2dbf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39180266a9594c238a41b9fbfc8c5204",
            "max": 1234829,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a81af7c89674d4e9ce254bcc587bc13",
            "value": 1234829
          }
        },
        "4a81af7c89674d4e9ce254bcc587bc13": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f51ffd0cd394ecdbebffc9c1d2ef4d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "545f468c00714ec1b4fe641898178723": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57550049c5414e069f850aa457ea9b83": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58d0c5456df84eee90ceda260c33b1d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59b2fd5f89414776a0e0a3fd34c722ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40382282dc4943ba8effdc629706b58f",
            "placeholder": "​",
            "style": "IPY_MODEL_4f51ffd0cd394ecdbebffc9c1d2ef4d0",
            "value": " 8.07k/? [00:00&lt;00:00, 392kB/s]"
          }
        },
        "6676d23c869c4c3ab62687963c9c7810": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a31c71fd829348feb18caa4bc63f4040",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_efbe83768ae34b1db83fa6cad104d45f",
            "value": 1
          }
        },
        "691df78c687140a8bbe9966657e599fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b5f9053c6434ef39b83f68713924c37": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f6c1a4b707747d49ebbc6a5d1eb3e99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f93dad37fd34852a7548ffd68dee957",
              "IPY_MODEL_d7ca6a3eccd841088c009a5cd1662065",
              "IPY_MODEL_b826516701c84219b7a527f3f68bc158"
            ],
            "layout": "IPY_MODEL_e1e2ddeae66c49d5ac1a8614cafbc360"
          }
        },
        "7e90ed1a82d04743a16636422c4b30c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e9d529fe1de4ff2a89882f256909c24": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7eef364a042d49118c9c0544820f2905": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8c4aecb573a14addb79bee4513b18307": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13a014a245ac4f1a887c242cca0b6809",
            "placeholder": "​",
            "style": "IPY_MODEL_6b5f9053c6434ef39b83f68713924c37",
            "value": "Generating test split: 100%"
          }
        },
        "8d29c9ba72b8475080a8a627cad79200": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e74726e602f48589ad8bf01901475ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8ea7f450ad7940c49c7e1579b1eef6c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8edd7d55e30f465bb808b6776a44c22f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e82f4c84d9b14d0c865b75e5d6fca012",
              "IPY_MODEL_6676d23c869c4c3ab62687963c9c7810",
              "IPY_MODEL_59b2fd5f89414776a0e0a3fd34c722ac"
            ],
            "layout": "IPY_MODEL_a5de794946254027a0d49afeb0892ec4"
          }
        },
        "8f1e7618846042f081ba5310042b54f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "987f8295f041452da4d749a55ce2c08e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9af480a284dd4450a2aff2f4a2b9c01c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd7fb66483a94eb6807450fa0913c9fd",
            "placeholder": "​",
            "style": "IPY_MODEL_987f8295f041452da4d749a55ce2c08e",
            "value": "data/test-00000-of-00001.parquet: 100%"
          }
        },
        "9cc5c22d97804947a1327190e5913e97": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a31c71fd829348feb18caa4bc63f4040": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "a5de794946254027a0d49afeb0892ec4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac5a30e347b34835806781b3d12dd934": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad6f2adea3dd4f32a238ff11cd1d5849",
            "max": 7600,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e74726e602f48589ad8bf01901475ff",
            "value": 7600
          }
        },
        "ad6f2adea3dd4f32a238ff11cd1d5849": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4d23888bfe24de3bd7b17b27a913ba2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58d0c5456df84eee90ceda260c33b1d6",
            "placeholder": "​",
            "style": "IPY_MODEL_cf0c230e554e47678d0743c3a2665f66",
            "value": " 18.6M/18.6M [00:02&lt;00:00, 15.8MB/s]"
          }
        },
        "b5c17b8684d544349ccce585e6ee4109": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b826516701c84219b7a527f3f68bc158": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e90ed1a82d04743a16636422c4b30c5",
            "placeholder": "​",
            "style": "IPY_MODEL_691df78c687140a8bbe9966657e599fb",
            "value": " 120000/120000 [00:00&lt;00:00, 539087.00 examples/s]"
          }
        },
        "c183984746ce46f3be1b7db347f529d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5e801c7db464bb98fed29d29a5dcd96",
            "max": 18585438,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7e9d529fe1de4ff2a89882f256909c24",
            "value": 18585438
          }
        },
        "cf0c230e554e47678d0743c3a2665f66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d12bb1f2387b4a54b8c284e926eba4d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_448db18dcb994dba838c83095efc1594",
            "placeholder": "​",
            "style": "IPY_MODEL_283b05d2e26140338cf7843ad986171d",
            "value": " 1.23M/1.23M [00:02&lt;00:00, 582kB/s]"
          }
        },
        "d5e801c7db464bb98fed29d29a5dcd96": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7ca6a3eccd841088c009a5cd1662065": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6ae47ae3f9849c4a543fff3a56344b0",
            "max": 120000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7eef364a042d49118c9c0544820f2905",
            "value": 120000
          }
        },
        "df80555aca904fc78569a6d9e6c012c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9af480a284dd4450a2aff2f4a2b9c01c",
              "IPY_MODEL_49e006e7e7b64b9790d1186bd7a2dbf3",
              "IPY_MODEL_d12bb1f2387b4a54b8c284e926eba4d5"
            ],
            "layout": "IPY_MODEL_57550049c5414e069f850aa457ea9b83"
          }
        },
        "e1e2ddeae66c49d5ac1a8614cafbc360": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e82f4c84d9b14d0c865b75e5d6fca012": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cc5c22d97804947a1327190e5913e97",
            "placeholder": "​",
            "style": "IPY_MODEL_8ea7f450ad7940c49c7e1579b1eef6c7",
            "value": "README.md: "
          }
        },
        "efbe83768ae34b1db83fa6cad104d45f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f66bda42670e498f96c415b054103d76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f8fa6ae187514f2393789cbc0e31d8f4",
              "IPY_MODEL_c183984746ce46f3be1b7db347f529d5",
              "IPY_MODEL_b4d23888bfe24de3bd7b17b27a913ba2"
            ],
            "layout": "IPY_MODEL_05e6bc29a6024c26998a8d38aa876108"
          }
        },
        "f6ae47ae3f9849c4a543fff3a56344b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f840c1d14bcf43bba1d61cc165f4f04d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8fa6ae187514f2393789cbc0e31d8f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d29c9ba72b8475080a8a627cad79200",
            "placeholder": "​",
            "style": "IPY_MODEL_1f0ccb8f46004bdf990b5e76ada71343",
            "value": "data/train-00000-of-00001.parquet: 100%"
          }
        },
        "fd7fb66483a94eb6807450fa0913c9fd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
